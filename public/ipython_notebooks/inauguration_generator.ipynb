{
 "metadata": {
  "name": "",
  "signature": "sha256:167c529a08afa487d4b541eb16580618c83d282864100ea19388bb2b5b3aaae4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Inauguration Address generator using Python "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To play around with text synthesis, I used the Natural Language Toolkit (nltk) library to generate an Inauguration Address. It works just about as well as you might imagine, its terrible. But it was pretty fun seeing what it came up with.\n",
      "\n",
      "The general method used here was something along the lines of a markov chain. At each state, we transition to the next with a probability that we have learned from training on the corpora. The random path that we take is used to form a sentence. \n",
      "\n",
      "The project is broken into two experiments. In the first the states are represented as words, and in the second the states are presentented by the word's part of speech tag (e.g noun, verb, etc.)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part One - Words as states"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This way of representing the states is the most common in speech synthesis. Its also pretty simple. The idea is that we build an adjacency structure for each word (or group of words) so we know what should follow it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from textblob import TextBlob\n",
      "from nltk.corpus import inaugural\n",
      "from collections import defaultdict\n",
      "from IPython.core.display import HTML"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll use start/stop tags to simplify the code. By sticking these on the start and end, we don't have to worry about edge cases so much"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_tag = 'START'\n",
      "end_tag   = 'STOP'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function generates the word-adjacency structure for a given corpus and order. Order here translates to the order of our markov chain. If the order is 1, then we look at the previous word to determine the next. If the order is 2, we look at the previous 2 words to determine the next and so on.\n",
      "\n",
      "There is also a struct parameter that we can pass a list or a set. The importance of this choice will be explained later"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_adjacency_map(corpus, order, struct=list):\n",
      "    adjacency_map = defaultdict(struct)    \n",
      "    for sentence in corpus.sents():        \n",
      "        # generate a list where the start and end are buffered by start and end tags.\n",
      "        words = ([start_tag] * order) + sentence + ([end_tag] * order)\n",
      "        \n",
      "        #slide over the list\n",
      "        #add each new word to the adjacency_map indexed by the preceding order-words\n",
      "        for i in range(0, len(sentence) + 1):\n",
      "            preceding_words = tuple([words[j] for j in range(i, i + order)])\n",
      "            word = words[i + order]\n",
      "            if struct is list:\n",
      "                adjacency_map[preceding_words].append(word)   \n",
      "            elif struct is set:\n",
      "                adjacency_map[preceding_words].add(word)  \n",
      "                \n",
      "    return adjacency_map         "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "word_adjacency_list = generate_adjacency_map(corpus = inaugural, order = 2, struct=list)\n",
      "word_adjacency_set = generate_adjacency_map(corpus = inaugural, order = 2, struct=set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course both structures contain the same number of unique elements, but in a set all unique words have equal probability to be picked at random. In a list, the more frequest the word, the more times it is expected to be picked at random. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print 'set : mean word adjacencies -', np.mean(map(len, word_adjacency_set.values()))\n",
      "print 'list: mean word adjacencies -', np.mean(map(len, word_adjacency_list.values()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set : mean word adjacencies - 1.81051536467\n",
        "list: mean word adjacencies - "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.33851958821\n"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function takes the adjacency structure and builds a random sentence out of it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_sentence(adjacency_map, order, max_len=50):\n",
      "    sentence = []\n",
      "    next_word = ''    \n",
      "    state = tuple([start_tag] * order)    \n",
      "    while next_word != end_tag and len(sentence) < max_len:\n",
      "        sentence.append(next_word)\n",
      "        next_word = np.random.choice(list(adjacency_map[state]))\n",
      "        state = state[1:] + (next_word,)\n",
      "    return ' '.join(sentence)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = build_sentence(word_adjacency_list, order=2)\n",
      "sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 150,
       "text": [
        "u' The currents of our Constitution , and the people , and history proves that when so adjusted as to foster a spirit of moderation and fairness .'"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part Two - Word tags as states"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, I had an idea. I could use the [TextBlob library](https://textblob.readthedocs.org/en/dev/) to tag the words and use that as the markov state instead. \n",
      "\n",
      "For each part of speech tag, I saved the assosiated bag of words, and built an adjacency structure similarly to what I did for words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_tag_adjacencies(corpus, order, struct=list):\n",
      "        \n",
      "    adjacency_map = defaultdict(struct)\n",
      "    tag_word_bags = defaultdict(struct, {end_tag : ['.']})\n",
      "\n",
      "    for sentence in corpus.sents(): \n",
      "        tagged_sent = TextBlob(' '.join(sentence)).tags    \n",
      "        \n",
      "        #add tagged words to the relevent tag bag\n",
      "        for word, tag in tagged_sent:\n",
      "            if struct is list:                \n",
      "                tag_word_bags[tag].append(word)\n",
      "            elif struct is set:\n",
      "                tag_word_bags[tag].add(word)\n",
      "\n",
      "        #build tag adjacency map similarly to the word-based one\n",
      "        tags = ([start_tag] * order) + map(lambda tup: tup[1], tagged_sent) + ([end_tag] * order)\n",
      "\n",
      "        for i in range(0, len(tagged_sent) + 1):\n",
      "            index = tuple([tags[j] for j in range(i, i + order)])\n",
      "            tag = tags[i + order]\n",
      "            if struct is list:                \n",
      "                adjacency_map[index].append(tag)\n",
      "            elif struct is set:\n",
      "                adjacency_map[index].add(tag)\n",
      "            \n",
      "    return adjacency_map, tag_word_bags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_adjacency_list, tag_bags_list = generate_tag_adjacencies(corpus = inaugural, order = 2, struct=list)\n",
      "tag_adjacency_set, tag_bags_set = generate_tag_adjacencies(corpus = inaugural, order = 2, struct=set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, the list/set difference makes a significant difference."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'list: mean tag adjacencies -', np.mean(map(len, tag_adjacency_list.values())), ', mean word bag size -', np.mean(map(len, tag_bags_list.values()))\n",
      "print 'set : mean tag adjacencies -', np.mean(map(len, tag_adjacency_set.values())), ', mean word bag size -', np.mean(map(len, tag_bags_set.values()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "list: mean tag adjacencies - 187.658469945 , mean word bag size - 3897.02941176\n",
        "set : mean tag adjacencies - 9.90300546448 , mean word bag size - 286.323529412\n"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This means that if we choose to use a set as the datastructure, our resulting sentences will feel much more random."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we build a sentence using the above structures similarly to before. But now the word we want to add to the sentence and what state we are entering are separate concepts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_tag_sentence(adjacency_map, tag_word_bags, order, max_len=50):\n",
      "    sentence = []\n",
      "    next_tag = None\n",
      "    state = tuple([start_tag] * order)\n",
      "    while next_tag != end_tag and len(sentence) < max_len:\n",
      "        next_tag = np.random.choice(list(adjacency_map[state]))\n",
      "        next_word = np.random.choice(list(tag_word_bags[next_tag]))\n",
      "        state = state[1:] + (next_tag,)\n",
      "        sentence.append(next_word)        \n",
      "    return ' '.join(sentence)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = build_tag_sentence(tag_adjacency_list, tag_bags_list, order = 2)\n",
      "sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 155,
       "text": [
        "u'every strong rights on the many temptation and out any Emperor .'"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we put together a bunch of sentences to make our Inaugural Address."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def output(sents):\n",
      "    return HTML('<p>' + ('</p><p>'.join(sents)) + '</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output([build_sentence(word_adjacency_list, order=2) for i in range(10)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p> If parties in personal actions the people is to be only the spiritual nature of man on Earth .</p><p> Take away from totalitarian darkness and to bear the cost of health care is too costly ; our commerce and culture , drawn from our new beginning is a grant from the Old World , with the determination to do it ?</p><p> As times change , against a sister republic , where internationality was to create those moral controls over blind economic forces and new plagues .</p><p> But while we do not hate ; we were not more than they were established they are members of the United States now enjoy .</p><p> To that world assembly of sovereign power over the services of the people .</p><p> Then I felt constrained to convene it under such circumstances I can summon from its opening hour until now , as time has passed away , to secure the negro race , or exile can know that self - government .</p><p> We sense with which they were invincible -- above and beyond the reach of mortal eye -- when the desire that it has left no injurious mark .</p><p> With high hope for instruction and furnishing abundant grounds for hopeful confidence , I know , that \" a little child shall lead them ,\" for our nation the way to permanent peace .</p><p> If revolution insists upon overturning established order , protect , and far more congenial with the great political family , and diligent exercise of our system without singular incongruity and the authority and increase it .</p><p> One of the Union , but we have wished to wrong from none has been arrested by the nursery for seamen and naval establishments .</p>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 157,
       "text": [
        "<IPython.core.display.HTML at 0x9d6e6a8c>"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output([build_tag_sentence(tag_adjacency_list, tag_bags_list, order = 2) for i in range(10)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p>We have would world such and clear the too fire to the resort they are now confined that those fair armies and All Government and to the .</p><p>it have the feelings of menace up our ages and by their effort to require any future not found the business .</p><p>has there fifty what forging civil still the threshold be it and to achieve a abnormal position that traditions to contemplate we be in the foreign many to smother .</p><p>me are made been by laws the most steady territory of CONCLUSION with no union present expense upon an should of justice my opinions of conduct We will certainly be sufficient by our great manufactures to involve good reasons Without I encourage human as field will height the combat by</p><p>America force of sound nor for our nation men from a property for their rates do undelayed of to the reservations in the calmly pretensions which with regarding itself .</p><p>All America enterprises .</p><p>The who write idea to lose service of the sum and from mutual nations .</p><p>Executive and here a principle of the selfish indispensable points It have were of those principle which have capable to policy our results and enhanced any most free of their need reasonable basis .</p><p>now have pursued his justice ' it exhilarated .</p><p>Ghent Government George In our people .</p>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 158,
       "text": [
        "<IPython.core.display.HTML at 0x9e5a2c0c>"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you might have expected, its just a nonsense generator. Increasing the order can reduce the nonsense-rating, but on a corpus as small as this, it ends up regurgitating entire sentences.\n",
      "\n",
      "The tag method seems to be even worse than the word-based method, but maybe you could combine them somehow to improve the result.\n",
      "\n",
      "[Download IPython Notebook](/public/ipython_notebooks/inauguration_generator.ipynb)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}